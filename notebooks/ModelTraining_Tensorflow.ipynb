{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "936b2c4e-ddf7-4e02-88e3-e280af0f2621",
   "metadata": {},
   "source": [
    "# Training a simple CNN model in Tensorflow for Tornado Detection\n",
    "\n",
    "This notebook steps through how to train a simple CNN model using a subset of TorNet.\n",
    "\n",
    "This will not produce a model with any skill, but simply provides a working end-to-end example of how to set up a data loader, build, and fit a model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "392369dc-ade9-4d34-8a5f-9d7d7d24a69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-25 15:37:38.337887: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-25 15:37:38.340166: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-25 15:37:38.385485: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-25 15:37:38.386856: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-25 15:37:39.216553: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# Uncomment if tornet isn't installed in your environment or in your path already\n",
    "#sys.path.append('../')  \n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tornet.data.tf.loader import create_tf_dataset \n",
    "from tornet.data.constants import ALL_VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "188f31eb-e051-4d5f-880d-7ef0a8eddcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create basic dataloader\n",
    "# This option loads directly from netcdf files, and will be slow and IO bound\n",
    "# To speed up training, either\n",
    "#     build as a tensorflow_dataset , (see tornet/data/tfds/tornet/README.md)\n",
    "#     cache dataset first , or\n",
    "#     use tf.data.Dataset.load on a pre-saved dataset\n",
    "\n",
    "# Location of tornet\n",
    "data_root = os.environ['TORNET_ROOT']\n",
    "\n",
    "# Get training data from 2018\n",
    "data_type='train'\n",
    "years = [2018,]\n",
    "\n",
    "catalog_path = os.path.join(data_root,'catalog.csv')\n",
    "if not os.path.exists(catalog_path):\n",
    "    raise RuntimeError('Unable to find catalog.csv at '+data_root)\n",
    "        \n",
    "catalog = pd.read_csv(catalog_path,parse_dates=['start_time','end_time'])\n",
    "catalog = catalog[catalog['type']==data_type]\n",
    "catalog = catalog[catalog.start_time.dt.year.isin(years)]\n",
    "catalog = catalog.sample(frac=1,random_state=1234)\n",
    "file_list = [os.path.join(data_root,f) for f in catalog.filename]\n",
    "\n",
    "ds = create_tf_dataset(file_list,variables=ALL_VARIABLES,n_frames=1) \n",
    "\n",
    "# (Optional) Save data for faster reloads (makes copy of data!)\n",
    "# ds.save('tornet_sample.tfdataset') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e33f8ffe-9e53-4970-bb08-a792fc185421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If saved with ds.save(...), just load that model\n",
    "#ds = tf.data.Dataset.load('tornet_sample.tfdataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "042a33b2-dffe-44d6-87ad-a966b6400504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If data was registered in tensorflow_dataset, use that\n",
    "# env variable TFDS_DATA_DIR should point to location of this resaved dataset\n",
    "#import tensorflow_datasets as tfds\n",
    "#import tornet.data.tfds.tornet.tornet_dataset_builder # registers 'tornet'\n",
    "\n",
    "#data_type='train'\n",
    "#years = [2018,]\n",
    "#ds = tfds.load('tornet',split='+'.join(['%s-%d' % (data_type,y) for y in years]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77ca60c9-9ac8-49ec-bce9-64d2610b6b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tornet.data.preprocess as pp\n",
    "from tornet.data.tf import preprocess as tfpp\n",
    "\n",
    "# Preprocess\n",
    "\n",
    "# add 'coordinates' variable used by CoordConv layers\n",
    "ds = ds.map(lambda d: pp.add_coordinates(d,include_az=False,backend=tf))\n",
    "     \n",
    "# Take only last time frame\n",
    "ds = ds.map(pp.remove_time_dim)\n",
    "\n",
    "# Split sample into inputs,label\n",
    "ds = ds.map(tfpp.split_x_y)\n",
    "\n",
    "# (Optional) add sample weights\n",
    "# weights={'wN':1.0,'w0':1.0,'w1':1.0,'w2':2.0,'wW':0.5}\n",
    "# ds = ds.map(lambda x,y:  tfpp.compute_sample_weight(x,y,**weights) )\n",
    "\n",
    "ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "ds = ds.batch(32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66d24ab7-0c41-4f14-864a-58d9831a2311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"TornadoDetector\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " DBZ (InputLayer)            [(None, 120, 240, 2)]        0         []                            \n",
      "                                                                                                  \n",
      " VEL (InputLayer)            [(None, 120, 240, 2)]        0         []                            \n",
      "                                                                                                  \n",
      " KDP (InputLayer)            [(None, 120, 240, 2)]        0         []                            \n",
      "                                                                                                  \n",
      " RHOHV (InputLayer)          [(None, 120, 240, 2)]        0         []                            \n",
      "                                                                                                  \n",
      " ZDR (InputLayer)            [(None, 120, 240, 2)]        0         []                            \n",
      "                                                                                                  \n",
      " WIDTH (InputLayer)          [(None, 120, 240, 2)]        0         []                            \n",
      "                                                                                                  \n",
      " Normalized_DBZ (Normalizat  (None, 120, 240, 2)          0         ['DBZ[0][0]']                 \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " Normalized_VEL (Normalizat  (None, 120, 240, 2)          0         ['VEL[0][0]']                 \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " Normalized_KDP (Normalizat  (None, 120, 240, 2)          0         ['KDP[0][0]']                 \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " Normalized_RHOHV (Normaliz  (None, 120, 240, 2)          0         ['RHOHV[0][0]']               \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " Normalized_ZDR (Normalizat  (None, 120, 240, 2)          0         ['ZDR[0][0]']                 \n",
      " ion)                                                                                             \n",
      "                                                                                                  \n",
      " Normalized_WIDTH (Normaliz  (None, 120, 240, 2)          0         ['WIDTH[0][0]']               \n",
      " ation)                                                                                           \n",
      "                                                                                                  \n",
      " Concatenate1 (Concatenate)  (None, 120, 240, 12)         0         ['Normalized_DBZ[0][0]',      \n",
      "                                                                     'Normalized_VEL[0][0]',      \n",
      "                                                                     'Normalized_KDP[0][0]',      \n",
      "                                                                     'Normalized_RHOHV[0][0]',    \n",
      "                                                                     'Normalized_ZDR[0][0]',      \n",
      "                                                                     'Normalized_WIDTH[0][0]']    \n",
      "                                                                                                  \n",
      " ReplaceNan (Lambda)         (None, 120, 240, 12)         0         ['Concatenate1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 120, 240, 32)         3488      ['ReplaceNan[0][0]']          \n",
      "                                                                                                  \n",
      " TornadoLikelihood (Conv2D)  (None, 120, 240, 1)          33        ['conv2d[0][0]']              \n",
      "                                                                                                  \n",
      " GlobalMaxPool (GlobalMaxPo  (None, 1)                    0         ['TornadoLikelihood[0][0]']   \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3521 (13.75 KB)\n",
      "Trainable params: 3521 (13.75 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create a simple CNN model\n",
    "# This normalizes data, concatenates along channel, and applies a Conv2D\n",
    "import keras\n",
    "from tornet.data.constants import CHANNEL_MIN_MAX\n",
    "\n",
    "input_vars = ALL_VARIABLES # which variables to use\n",
    "\n",
    "# TF convention is B,L,W,H\n",
    "inputs = {v:keras.Input(shape=(120,240,2),name=v) for v in input_vars}\n",
    "\n",
    "# Normalize inputs\n",
    "norm_layers = []\n",
    "for v in input_vars:\n",
    "    min_max = np.array(CHANNEL_MIN_MAX[v]) # [2,]\n",
    "\n",
    "    # choose mean,var to get approximate [-1,1] scaling\n",
    "    var=((min_max[1]-min_max[0])/2)**2 # scalar\n",
    "    var=np.array(2*[var,])    # [n_sweeps,]\n",
    "    offset=(min_max[0]+min_max[1])/2    # scalar\n",
    "    offset=np.array(2*[offset,]) # [n_sweeps,]\n",
    "    \n",
    "    norm_layers.append(\n",
    "        keras.layers.Normalization(mean=offset, variance=var,\n",
    "                                   name='Normalized_%s' % v)\n",
    "    )\n",
    "\n",
    "# Concatenate normed inputs along channel dimension\n",
    "x=keras.layers.Concatenate(axis=-1,name='Concatenate1')(\n",
    "        [l(inputs[v]) for l,v in zip(norm_layers,input_vars)]\n",
    "        )\n",
    "\n",
    "# Replace background (nan) with -3\n",
    "x=keras.layers.Lambda(lambda x: tf.where(tf.math.is_nan(x),-3.0,x),name='ReplaceNan')(x)\n",
    "\n",
    "# Processing\n",
    "x = keras.layers.Conv2D(32,3,padding='same',activation='relu')(x)\n",
    "# add more..\n",
    "x = keras.layers.Conv2D(1,1,padding='same',activation='relu',name='TornadoLikelihood')(x)\n",
    "y = keras.layers.GlobalMaxPool2D(name='GlobalMaxPool')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs,outputs=y,name='TornadoDetector')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11dfbfa0-3cdc-4676-864e-f7cbcd869443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "opt  = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss=keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "model.compile(loss=loss, optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f402213-58a7-44be-a1cb-4d0aa16428d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/llgrid/pkg/anaconda/anaconda3-2023b/lib/python3.9/site-packages/keras/src/engine/functional.py:639: UserWarning: Input dict contained keys ['range_folded_mask', 'category', 'event_id', 'ef_number', 'az_lower', 'az_upper', 'rng_lower', 'rng_upper', 'time', 'coordinates'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 17s 2s/step - loss: 1.2716\n",
      "Epoch 2/3\n",
      "10/10 [==============================] - 16s 2s/step - loss: 0.7283\n",
      "Epoch 3/3\n",
      "10/10 [==============================] - 16s 2s/step - loss: 0.6942\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f232b41d9a0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "\n",
    "# steps_per_epoch=10 for demo purposes\n",
    "model.fit(ds,epochs=3,steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae798b07-9ac5-4784-ba44-cfa242649e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a test set\n",
    "# Basic loader\n",
    "data_type='test'\n",
    "years = [2018]\n",
    "\n",
    "catalog_path = os.path.join(data_root,'catalog.csv')\n",
    "if not os.path.exists(catalog_path):\n",
    "    raise RuntimeError('Unable to find catalog.csv at '+data_root)\n",
    "        \n",
    "catalog = pd.read_csv(catalog_path,parse_dates=['start_time','end_time'])\n",
    "catalog = catalog[catalog['type']==data_type]\n",
    "catalog = catalog[catalog.start_time.dt.year.isin(years)]\n",
    "catalog = catalog.sample(frac=1,random_state=1234)\n",
    "file_list = [os.path.join(data_root,f) for f in catalog.filename]\n",
    "\n",
    "ds_test = create_tf_dataset(file_list,variables=ALL_VARIABLES,n_frames=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61d46e5e-fbef-42f9-af95-29bc8bfbc171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFDS loader\n",
    "# env variable TFDS_DATA_DIR should point to location of resaved dataset\n",
    "#import tensorflow_datasets as tfds\n",
    "#import tornet.data.tfds.tornet.tornet_dataset_builder # registers 'tornet'\n",
    "\n",
    "#data_type='test'\n",
    "#years = [2018,]\n",
    "#ds_test = tfds.load('tornet',split='+'.join(['%s-%d' % (data_type,y) for y in years]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84a3261c-5085-43c9-84bb-febae18daeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "ds_test = ds_test.map(lambda d: pp.add_coordinates(d,include_az=False,backend=tf))\n",
    "ds_test = ds_test.map(pp.remove_time_dim)\n",
    "ds_test = ds_test.map(tfpp.split_x_y)\n",
    "ds_test = ds_test.prefetch(tf.data.AUTOTUNE)    \n",
    "ds_test = ds_test.batch(32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6af44de5-af4e-438e-a0b1-153fbe72be84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/llgrid/pkg/anaconda/anaconda3-2023b/lib/python3.9/site-packages/keras/src/engine/functional.py:639: UserWarning: Input dict contained keys ['range_folded_mask', 'category', 'event_id', 'ef_number', 'az_lower', 'az_upper', 'rng_lower', 'rng_upper', 'time', 'coordinates'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 17s 2s/step - loss: 0.6938 - AUC: 0.5249 - BinaryAccuracy: 0.9406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6937962770462036, 0.524876594543457, 0.940625011920929]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate\n",
    "import tornet.metrics.tf.metrics as km\n",
    "metrics = [km.AUC(from_logits=True,name='AUC'),\n",
    "           km.BinaryAccuracy(from_logits=True,name='BinaryAccuracy'), \n",
    "           ]\n",
    "model.compile(loss=loss,metrics=metrics)\n",
    "\n",
    "# steps=10 for demo purposes\n",
    "model.evaluate(ds_test,steps=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
