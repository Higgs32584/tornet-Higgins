{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "936b2c4e-ddf7-4e02-88e3-e280af0f2621",
   "metadata": {},
   "source": [
    "# Training a simple CNN model in Tensorflow for Tornado Detection\n",
    "\n",
    "This notebook steps through how to train a simple CNN model using a subset of TorNet.\n",
    "\n",
    "This will not produce a model with any skill, but simply provides a working end-to-end example of how to set up a data loader, build, and fit a model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392369dc-ade9-4d34-8a5f-9d7d7d24a69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Uncomment if tornet isn't installed in your environment or in your path already\n",
    "#sys.path.append('../')  \n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tornet.data.tf.loader import create_tf_dataset \n",
    "from tornet.data.constants import ALL_VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188f31eb-e051-4d5f-880d-7ef0a8eddcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create basic dataloader\n",
    "# This option loads directly from netcdf files, and will be slow and IO bound\n",
    "# To speed up training, either\n",
    "#     build as a tensorflow_dataset , (see tornet/data/tfds/tornet/README.md)\n",
    "#     cache dataset first , or\n",
    "#     use tf.data.Dataset.load on a pre-saved dataset\n",
    "\n",
    "# Location of tornet\n",
    "data_root = os.environ['TORNET_ROOT']\n",
    "\n",
    "# Get training data from 2018\n",
    "data_type='train'\n",
    "years = [2018,]\n",
    "\n",
    "catalog_path = os.path.join(data_root,'catalog.csv')\n",
    "if not os.path.exists(catalog_path):\n",
    "    raise RuntimeError('Unable to find catalog.csv at '+data_root)\n",
    "        \n",
    "catalog = pd.read_csv(catalog_path,parse_dates=['start_time','end_time'])\n",
    "catalog = catalog[catalog['type']==data_type]\n",
    "catalog = catalog[catalog.start_time.dt.year.isin(years)]\n",
    "catalog = catalog.sample(frac=1,random_state=1234)\n",
    "file_list = [os.path.join(data_root,f) for f in catalog.filename]\n",
    "\n",
    "ds = create_tf_dataset(file_list,variables=ALL_VARIABLES,n_frames=1) \n",
    "\n",
    "# (Optional) Save data for faster reloads (makes copy of data!)\n",
    "# ds.save('tornet_sample.tfdataset') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33f8ffe-9e53-4970-bb08-a792fc185421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If saved with ds.save(...), just load that model\n",
    "#ds = tf.data.Dataset.load('tornet_sample.tfdataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042a33b2-dffe-44d6-87ad-a966b6400504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If data was registered in tensorflow_dataset, use that\n",
    "# env variable TFDS_DATA_DIR should point to location of this resaved dataset\n",
    "#import tensorflow_datasets as tfds\n",
    "#import tornet.data.tfds.tornet.tornet_dataset_builder # registers 'tornet'\n",
    "\n",
    "#data_type='train'\n",
    "#years = [2018,]\n",
    "#ds = tfds.load('tornet',split='+'.join(['%s-%d' % (data_type,y) for y in years]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ca60c9-9ac8-49ec-bce9-64d2610b6b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tornet.data.preprocess as pp\n",
    "from tornet.data import preprocess as tfpp\n",
    "\n",
    "# Preprocess\n",
    "\n",
    "# add 'coordinates' variable used by CoordConv layers\n",
    "ds = ds.map(lambda d: pp.add_coordinates(d,include_az=False,backend=tf))\n",
    "     \n",
    "# Take only last time frame\n",
    "ds = ds.map(pp.remove_time_dim)\n",
    "\n",
    "# Split sample into inputs,label\n",
    "ds = ds.map(tfpp.split_x_y)\n",
    "\n",
    "# (Optional) add sample weights\n",
    "# weights={'wN':1.0,'w0':1.0,'w1':1.0,'w2':2.0,'wW':0.5}\n",
    "# ds = ds.map(lambda x,y:  tfpp.compute_sample_weight(x,y,**weights) )\n",
    "\n",
    "ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "ds = ds.batch(32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d24ab7-0c41-4f14-864a-58d9831a2311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple CNN model\n",
    "# This normalizes data, concatenates along channel, and applies a Conv2D\n",
    "import keras\n",
    "from tornet.data.constants import CHANNEL_MIN_MAX\n",
    "\n",
    "input_vars = ALL_VARIABLES # which variables to use\n",
    "\n",
    "# TF convention is B,L,W,H\n",
    "inputs = {v:keras.Input(shape=(120,240,2),name=v) for v in input_vars}\n",
    "\n",
    "# Normalize inputs\n",
    "norm_layers = []\n",
    "for v in input_vars:\n",
    "    min_max = np.array(CHANNEL_MIN_MAX[v]) # [2,]\n",
    "\n",
    "    # choose mean,var to get approximate [-1,1] scaling\n",
    "    var=((min_max[1]-min_max[0])/2)**2 # scalar\n",
    "    var=np.array(2*[var,])    # [n_sweeps,]\n",
    "    offset=(min_max[0]+min_max[1])/2    # scalar\n",
    "    offset=np.array(2*[offset,]) # [n_sweeps,]\n",
    "    \n",
    "    norm_layers.append(\n",
    "        keras.layers.Normalization(mean=offset, variance=var,\n",
    "                                   name='Normalized_%s' % v)\n",
    "    )\n",
    "\n",
    "# Concatenate normed inputs along channel dimension\n",
    "x=keras.layers.Concatenate(axis=-1,name='Concatenate1')(\n",
    "        [l(inputs[v]) for l,v in zip(norm_layers,input_vars)]\n",
    "        )\n",
    "\n",
    "# Replace background (nan) with -3\n",
    "x=keras.layers.Lambda(lambda x: tf.where(tf.math.is_nan(x),-3.0,x),name='ReplaceNan')(x)\n",
    "\n",
    "# Processing\n",
    "x = keras.layers.Conv2D(32,3,padding='same',activation='relu')(x)\n",
    "# add more..\n",
    "x = keras.layers.Conv2D(1,1,padding='same',activation='relu',name='TornadoLikelihood')(x)\n",
    "y = keras.layers.GlobalMaxPool2D(name='GlobalMaxPool')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs,outputs=y,name='TornadoDetector')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dfbfa0-3cdc-4676-864e-f7cbcd869443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "opt  = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss=keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "model.compile(loss=loss, optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f402213-58a7-44be-a1cb-4d0aa16428d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "\n",
    "# steps_per_epoch=10 for demo purposes\n",
    "model.fit(ds,epochs=3,steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae798b07-9ac5-4784-ba44-cfa242649e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a test set\n",
    "# Basic loader\n",
    "data_type='test'\n",
    "years = [2018]\n",
    "\n",
    "catalog_path = os.path.join(data_root,'catalog.csv')\n",
    "if not os.path.exists(catalog_path):\n",
    "    raise RuntimeError('Unable to find catalog.csv at '+data_root)\n",
    "        \n",
    "catalog = pd.read_csv(catalog_path,parse_dates=['start_time','end_time'])\n",
    "catalog = catalog[catalog['type']==data_type]\n",
    "catalog = catalog[catalog.start_time.dt.year.isin(years)]\n",
    "catalog = catalog.sample(frac=1,random_state=1234)\n",
    "file_list = [os.path.join(data_root,f) for f in catalog.filename]\n",
    "\n",
    "ds_test = create_tf_dataset(file_list,variables=ALL_VARIABLES,n_frames=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d46e5e-fbef-42f9-af95-29bc8bfbc171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFDS loader\n",
    "# env variable TFDS_DATA_DIR should point to location of resaved dataset\n",
    "#import tensorflow_datasets as tfds\n",
    "#import tornet.data.tfds.tornet.tornet_dataset_builder # registers 'tornet'\n",
    "\n",
    "#data_type='test'\n",
    "#years = [2018,]\n",
    "#ds_test = tfds.load('tornet',split='+'.join(['%s-%d' % (data_type,y) for y in years]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a3261c-5085-43c9-84bb-febae18daeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "ds_test = ds_test.map(lambda d: pp.add_coordinates(d,include_az=False,backend=tf))\n",
    "ds_test = ds_test.map(pp.remove_time_dim)\n",
    "ds_test = ds_test.map(tfpp.split_x_y)\n",
    "ds_test = ds_test.prefetch(tf.data.AUTOTUNE)    \n",
    "ds_test = ds_test.batch(32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af44de5-af4e-438e-a0b1-153fbe72be84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "import tornet.metrics.keras.metrics as km\n",
    "metrics = [keras.metrics.AUC(from_logits=True,name='AUC'),\n",
    "           km.BinaryAccuracy(from_logits=True,name='BinaryAccuracy'), \n",
    "           ]\n",
    "model.compile(loss=loss,metrics=metrics)\n",
    "\n",
    "# steps=10 for demo purposes\n",
    "model.evaluate(ds_test,steps=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
