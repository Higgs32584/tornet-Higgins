{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Uncomment if tornet isn't installed in your environment or in your path already\n",
    "#sys.path.append('../')  \n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "\n",
    "from tornet.data.tf.loader import create_tf_dataset \n",
    "from tornet.data.constants import ALL_VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras accepts most data loaders (tensorflow, torch).\n",
    "# A pure keras data loader, with necessary preprocessing steps for the cnn baseline, is provided\n",
    "from tornet.data.keras.loader import KerasDataLoader\n",
    "data_root = \"C:/Users/mjhig/tornet_2013\"\n",
    "ds = KerasDataLoader(data_root=data_root,\n",
    "                     data_type='train',\n",
    "                     years=[2013],\n",
    "                     batch_size = 8, \n",
    "                     workers = 4,\n",
    "                     use_multiprocessing = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a test set\n",
    "ds_test = KerasDataLoader(data_root=data_root,\n",
    "                         data_type='test',\n",
    "                         years=[2018],\n",
    "                         batch_size = 8, \n",
    "                         workers = 4,\n",
    "                         use_multiprocessing = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 438/438 [07:16<00:00,  1.00batch/s]\n",
      "Processing Batches:   2%|▏         | 7/315 [00:17<12:48,  2.50s/batch]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\backends\\file_manager.py:211\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[1;34m(self, needs_lock)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 211\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\backends\\lru_cache.py:56\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m---> 56\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mmove_to_end(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('C:\\\\Users\\\\mjhig\\\\tornet_2013\\\\test\\\\2018\\\\NUL_180717_225211_KDDC_776259s_K1.nc',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), '0659135f-a9ca-4c8c-9a18-83020aef69dc']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m     29\u001b[0m X, y \u001b[38;5;241m=\u001b[39m get_all_batches_for_randomforest(ds, ALL_VARIABLES)\n\u001b[1;32m---> 30\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mget_all_batches_for_randomforest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mALL_VARIABLES\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m, in \u001b[0;36mget_all_batches_for_randomforest\u001b[1;34m(dataset, input_vars)\u001b[0m\n\u001b[0;32m     10\u001b[0m X, y \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m     12\u001b[0m total_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset)  \u001b[38;5;66;03m# Get total batch count\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(dataset, total\u001b[38;5;241m=\u001b[39mtotal_batches, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing Batches\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# Concatenate selected input variables along the last axis\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([batch[\u001b[38;5;241m0\u001b[39m][v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m input_vars], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Reshape features: (batch_size, feature_dim_1, feature_dim_2, ...) → (batch_size, num_features)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\tornet\\data\\keras\\loader.py:108\u001b[0m, in \u001b[0;36mKerasDataLoader.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    106\u001b[0m element_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files_batch:\n\u001b[1;32m--> 108\u001b[0m     element_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mALL_VARIABLES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtilt_last\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtilt_last\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# Transforms\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m element_list:\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\tornet\\data\\loader.py:47\u001b[0m, in \u001b[0;36mread_file\u001b[1;34m(f, variables, n_frames, tilt_last)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03mExtracts data from a single netcdf file\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03mDict containing data for each variable, along with several metadata fields.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     46\u001b[0m data \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m ds:\n\u001b[0;32m     48\u001b[0m     \n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# Load each radar variable\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m variables:\n\u001b[0;32m     51\u001b[0m         data[v]\u001b[38;5;241m=\u001b[39mds[v]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m-\u001b[39mn_frames:,:,:,:]\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\backends\\api.py:566\u001b[0m, in \u001b[0;36mopen_dataset\u001b[1;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    554\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[0;32m    555\u001b[0m     decode_cf,\n\u001b[0;32m    556\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    562\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[0;32m    563\u001b[0m )\n\u001b[0;32m    565\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 566\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mopen_dataset(\n\u001b[0;32m    567\u001b[0m     filename_or_obj,\n\u001b[0;32m    568\u001b[0m     drop_variables\u001b[38;5;241m=\u001b[39mdrop_variables,\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecoders,\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    571\u001b[0m )\n\u001b[0;32m    572\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[0;32m    573\u001b[0m     backend_ds,\n\u001b[0;32m    574\u001b[0m     filename_or_obj,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    585\u001b[0m )\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:590\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.open_dataset\u001b[1;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, lock, autoclose)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_dataset\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]  # allow LSP violation, not supporting **kwargs\u001b[39;00m\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    571\u001b[0m     filename_or_obj: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m os\u001b[38;5;241m.\u001b[39mPathLike[Any] \u001b[38;5;241m|\u001b[39m BufferedIOBase \u001b[38;5;241m|\u001b[39m AbstractDataStore,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    587\u001b[0m     autoclose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    588\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[0;32m    589\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[1;32m--> 590\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[43mNetCDF4DataStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclobber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclobber\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiskless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiskless\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    602\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:391\u001b[0m, in \u001b[0;36mNetCDF4DataStore.open\u001b[1;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001b[0m\n\u001b[0;32m    385\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    386\u001b[0m     clobber\u001b[38;5;241m=\u001b[39mclobber, diskless\u001b[38;5;241m=\u001b[39mdiskless, persist\u001b[38;5;241m=\u001b[39mpersist, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m\n\u001b[0;32m    387\u001b[0m )\n\u001b[0;32m    388\u001b[0m manager \u001b[38;5;241m=\u001b[39m CachingFileManager(\n\u001b[0;32m    389\u001b[0m     netCDF4\u001b[38;5;241m.\u001b[39mDataset, filename, mode\u001b[38;5;241m=\u001b[39mmode, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[0;32m    390\u001b[0m )\n\u001b[1;32m--> 391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:338\u001b[0m, in \u001b[0;36mNetCDF4DataStore.__init__\u001b[1;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group \u001b[38;5;241m=\u001b[39m group\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m--> 338\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[38;5;241m.\u001b[39mdata_model\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mfilepath()\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_remote \u001b[38;5;241m=\u001b[39m is_remote_uri(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename)\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:400\u001b[0m, in \u001b[0;36mNetCDF4DataStore.ds\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:394\u001b[0m, in \u001b[0;36mNetCDF4DataStore._acquire\u001b[1;34m(self, needs_lock)\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 394\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39macquire_context(needs_lock) \u001b[38;5;28;01mas\u001b[39;00m root:\n\u001b[0;32m    395\u001b[0m         ds \u001b[38;5;241m=\u001b[39m _nc4_require_group(root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode)\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\backends\\file_manager.py:199\u001b[0m, in \u001b[0;36mCachingFileManager.acquire_context\u001b[1;34m(self, needs_lock)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    198\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m     file, cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire_with_cache_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\backends\\file_manager.py:217\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[1;34m(self, needs_lock)\u001b[0m\n\u001b[0;32m    215\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    216\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\n\u001b[1;32m--> 217\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opener(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32msrc\\\\netCDF4\\\\_netCDF4.pyx:2492\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\netCDF4\\\\_netCDF4.pyx:2009\u001b[0m, in \u001b[0;36mgenexpr\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\netCDF4\\\\_netCDF4.pyx:2009\u001b[0m, in \u001b[0;36mgenexpr\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\netCDF4\\utils.py:32\u001b[0m, in \u001b[0;36m_find_dim\u001b[1;34m(grp, dimname)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sortbylist\u001b[39m(A,B):\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# sort one list (A) using the values from another list (B)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [A[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(A)), key\u001b[38;5;241m=\u001b[39mB\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m)]\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_find_dim\u001b[39m(grp, dimname):\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# find Dimension instance given group and name.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# look in current group, and parents.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     group \u001b[38;5;241m=\u001b[39m grp\n\u001b[0;32m     36\u001b[0m     dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Split data into training and validation sets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, accuracy_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_all_batches_for_randomforest(dataset, input_vars):\n",
    "    \"\"\"Extracts all batches, flattens them, and formats them for RandomForestClassifier.\"\"\"\n",
    "    X, y = [], []\n",
    "    \n",
    "    total_batches = len(dataset)  # Get total batch count\n",
    "    \n",
    "    for batch in tqdm(dataset, total=total_batches, desc=\"Processing Batches\", unit=\"batch\"):\n",
    "        # Concatenate selected input variables along the last axis\n",
    "        features = np.concatenate([batch[0][v] for v in input_vars], axis=-1)\n",
    "        \n",
    "        # Reshape features: (batch_size, feature_dim_1, feature_dim_2, ...) → (batch_size, num_features)\n",
    "        X.append(features.reshape(features.shape[0], -1))  # Flatten to 2D\n",
    "        y.append(batch[1])\n",
    "\n",
    "    # Stack all collected batches\n",
    "    X = np.vstack(X)  # Stack features\n",
    "    y = np.hstack(y)  # Stack labels\n",
    "    \n",
    "    return X, y  # Ready for RandomForestClassifier\n",
    "\n",
    "# Example usage:\n",
    "X, y = get_all_batches_for_randomforest(ds, ALL_VARIABLES)\n",
    "X_test, y_test = get_all_batches_for_randomforest(ds_test, ALL_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train-test split\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# # Train RandomForestClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# rf_model = RandomForestClassifier(\n",
    "#     n_estimators=100,  # Number of trees\n",
    "#     max_depth=10,      # Limit depth to prevent overfitting\n",
    "#     random_state=42,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "# rf_model.fit(X, y)\n",
    "# print(\"RandomForest Model Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, accuracy_score\n",
    "\n",
    "# Define models with hyperparameters\n",
    "models = {\n",
    "    \"SVM\": SVC(kernel=\"rbf\", C=1.0, random_state=42, probability=True),  # Enable probability estimates\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "}\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Compute Metrics\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"{name} - Validation AUC: {roc_auc:.4f}\")\n",
    "    print(f\"{name} - Validation PR AUC: {pr_auc:.4f}\")\n",
    "    print(f\"{name} - Validation Accuracy: {accuracy:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tornet-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
