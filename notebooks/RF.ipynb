{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Uncomment if tornet isn't installed in your environment or in your path already\n",
    "#sys.path.append('../')  \n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "\n",
    "from tornet.data.tf.loader import create_tf_dataset \n",
    "from tornet.data.constants import ALL_VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras accepts most data loaders (tensorflow, torch).\n",
    "# A pure keras data loader, with necessary preprocessing steps for the cnn baseline, is provided\n",
    "from tornet.data.keras.loader import KerasDataLoader\n",
    "data_root = \"C:/Users/mjhig/tornet_2013\"\n",
    "ds = KerasDataLoader(data_root=data_root,\n",
    "                     data_type='train',\n",
    "                     years=[2013, 2014, 2015, 2016, 2017, 2018],\n",
    "                     batch_size = 128, \n",
    "                     workers = 4,\n",
    "                     use_multiprocessing = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a test set\n",
    "ds_test = KerasDataLoader(data_root=data_root,\n",
    "                         data_type='test',\n",
    "                         years=[2018],\n",
    "                         batch_size = 128, \n",
    "                         workers = 4,\n",
    "                         use_multiprocessing = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_first, y_first = ds[0]  \n",
    "unique_classes = np.unique(y_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y=ds[0]\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['DBZ', 'VEL', 'KDP', 'RHOHV', 'ZDR', 'WIDTH'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x={ k: x[k] for k in ALL_VARIABLES }\n",
    "x.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 345600)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch, y_batch = ds[0]\n",
    "X_batch = np.stack([X_batch[k] for k in ALL_VARIABLES], axis=-1)\n",
    "X_batch = X_batch.reshape(X_batch.shape[0], -1)\n",
    "X_batch.shape\n",
    "#classifier.partial_fit(X_batch, y_batch, classes=unique_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m X_batch \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([X_batch[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m ALL_VARIABLES], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     11\u001b[0m X_batch \u001b[38;5;241m=\u001b[39m X_batch\u001b[38;5;241m.\u001b[39mreshape(X_batch\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m done\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\gradient_boosting.py:663\u001b[0m, in \u001b[0;36mBaseHistGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    655\u001b[0m n_bins \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_bins \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# + 1 for missing values\u001b[39;00m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bin_mapper \u001b[38;5;241m=\u001b[39m _BinMapper(\n\u001b[0;32m    657\u001b[0m     n_bins\u001b[38;5;241m=\u001b[39mn_bins,\n\u001b[0;32m    658\u001b[0m     is_categorical\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_categorical_remapped,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    661\u001b[0m     n_threads\u001b[38;5;241m=\u001b[39mn_threads,\n\u001b[0;32m    662\u001b[0m )\n\u001b[1;32m--> 663\u001b[0m X_binned_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bin_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_training_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_val \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    665\u001b[0m     X_binned_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bin_data(X_val, is_training_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\gradient_boosting.py:1178\u001b[0m, in \u001b[0;36mBaseHistGradientBoosting._bin_data\u001b[1;34m(self, X, is_training_data)\u001b[0m\n\u001b[0;32m   1176\u001b[0m tic \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_training_data:\n\u001b[1;32m-> 1178\u001b[0m     X_binned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bin_mapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# F-aligned array\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1180\u001b[0m     X_binned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bin_mapper\u001b[38;5;241m.\u001b[39mtransform(X)  \u001b[38;5;66;03m# F-aligned array\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\sklearn\\base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    903\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    904\u001b[0m             (\n\u001b[0;32m    905\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    913\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m    914\u001b[0m         )\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\binning.py:234\u001b[0m, in \u001b[0;36m_BinMapper.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbin_thresholds_ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m n_features\n\u001b[0;32m    232\u001b[0m n_bins_non_missing \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m n_features\n\u001b[1;32m--> 234\u001b[0m non_cat_thresholds \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreading\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_find_binning_thresholds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_bins\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_categorical_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m non_cat_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_features):\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.impute import SimpleImputer\n",
    "from \n",
    "\n",
    "SGDClassifier\n",
    "classifier = SGDClassifier()\n",
    "X_first, y_first = ds[0]  \n",
    "unique_classes = np.unique(y_first)\n",
    "for i in range(1, len(ds)):  # Start from batch 1\n",
    "    X_batch, y_batch = ds[i]\n",
    "    X_batch = np.stack([X_batch[k] for k in ALL_VARIABLES], axis=-1)\n",
    "    X_batch = X_batch.reshape(X_batch.shape[0], -1)\n",
    "    classifier.fit(X_batch, y_batch)\n",
    "    print(f\"Batch {i} done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 prediction done\n",
      "Batch 1 prediction done\n",
      "Batch 2 prediction done\n",
      "Batch 3 prediction done\n",
      "Batch 4 prediction done\n",
      "Batch 5 prediction done\n",
      "Batch 6 prediction done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Iterate through test dataset batches\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ds_test)):  \n\u001b[1;32m---> 10\u001b[0m     X_batch, y_batch \u001b[38;5;241m=\u001b[39m \u001b[43mds_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Load batch\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Extract and stack only relevant features\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     X_batch \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([X_batch[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m ALL_VARIABLES], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\tornet\\data\\keras\\loader.py:108\u001b[0m, in \u001b[0;36mKerasDataLoader.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    106\u001b[0m element_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files_batch:\n\u001b[1;32m--> 108\u001b[0m     element_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mALL_VARIABLES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtilt_last\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtilt_last\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# Transforms\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m element_list:\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\tornet\\data\\loader.py:51\u001b[0m, in \u001b[0;36mread_file\u001b[1;34m(f, variables, n_frames, tilt_last)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m xr\u001b[38;5;241m.\u001b[39mopen_dataset(f) \u001b[38;5;28;01mas\u001b[39;00m ds:\n\u001b[0;32m     48\u001b[0m     \n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# Load each radar variable\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m variables:\n\u001b[1;32m---> 51\u001b[0m         data[v]\u001b[38;5;241m=\u001b[39m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m[\u001b[38;5;241m-\u001b[39mn_frames:,:,:,:]\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;66;03m# Various numeric metadata\u001b[39;00m\n\u001b[0;32m     54\u001b[0m     data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrange_folded_mask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrange_folded_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m-\u001b[39mn_frames:,:,:,:]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;66;03m# only two channels for vel,width\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\core\\dataarray.py:733\u001b[0m, in \u001b[0;36mDataArray.values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m    726\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;124;03m    The array's data as a numpy.ndarray.\u001b[39;00m\n\u001b[0;32m    728\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;124;03m    type does not support coercion like this (e.g. cupy).\u001b[39;00m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\core\\variable.py:614\u001b[0m, in \u001b[0;36mVariable.values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    613\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The variable's data as a numpy.ndarray\"\"\"\u001b[39;00m\n\u001b[1;32m--> 614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_as_array_or_item\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\core\\variable.py:314\u001b[0m, in \u001b[0;36m_as_array_or_item\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_as_array_or_item\u001b[39m(data):\n\u001b[0;32m    301\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the given values as a numpy array, or as an individual item if\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;124;03m    it's a 0d datetime64 or timedelta64 array.\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;124;03m    TODO: remove this (replace with np.asarray) once these issues are fixed\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    316\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\core\\indexing.py:693\u001b[0m, in \u001b[0;36mMemoryCachedArray.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: np\u001b[38;5;241m.\u001b[39mtyping\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m--> 693\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\core\\indexing.py:696\u001b[0m, in \u001b[0;36mMemoryCachedArray.get_duck_array\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 696\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    697\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray\u001b[38;5;241m.\u001b[39mget_duck_array()\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\core\\indexing.py:690\u001b[0m, in \u001b[0;36mMemoryCachedArray._ensure_cached\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    689\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ensure_cached\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 690\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray \u001b[38;5;241m=\u001b[39m as_indexable(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\core\\indexing.py:664\u001b[0m, in \u001b[0;36mCopyOnWriteArray.get_duck_array\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\core\\indexing.py:557\u001b[0m, in \u001b[0;36mLazilyIndexedArray.get_duck_array\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;66;03m# self.array[self.key] is now a numpy array when\u001b[39;00m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;66;03m# self.array is a BackendArray subclass\u001b[39;00m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;66;03m# and self.key is BasicIndexer((slice(None, None, None),))\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;66;03m# so we need the explicit check for ExplicitlyIndexed\u001b[39;00m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, ExplicitlyIndexed):\n\u001b[1;32m--> 557\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrap_numpy_scalars(array)\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\coding\\variables.py:74\u001b[0m, in \u001b[0;36m_ElementwiseFunctionArray.get_duck_array\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\core\\indexing.py:551\u001b[0m, in \u001b[0;36mLazilyIndexedArray.get_duck_array\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 551\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;66;03m# self.array[self.key] is now a numpy array when\u001b[39;00m\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;66;03m# self.array is a BackendArray subclass\u001b[39;00m\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;66;03m# and self.key is BasicIndexer((slice(None, None, None),))\u001b[39;00m\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;66;03m# so we need the explicit check for ExplicitlyIndexed\u001b[39;00m\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, ExplicitlyIndexed):\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:100\u001b[0m, in \u001b[0;36mNetCDF4ArrayWrapper.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mindexing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplicit_indexing_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIndexingSupport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOUTER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\core\\indexing.py:858\u001b[0m, in \u001b[0;36mexplicit_indexing_adapter\u001b[1;34m(key, shape, indexing_support, raw_indexing_method)\u001b[0m\n\u001b[0;32m    836\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Support explicit indexing by delegating to a raw indexing method.\u001b[39;00m\n\u001b[0;32m    837\u001b[0m \n\u001b[0;32m    838\u001b[0m \u001b[38;5;124;03mOuter and/or vectorized indexers are supported by indexing a second time\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;124;03mIndexing result, in the form of a duck numpy-array.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    857\u001b[0m raw_key, numpy_indices \u001b[38;5;241m=\u001b[39m decompose_indexer(key, shape, indexing_support)\n\u001b[1;32m--> 858\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mraw_indexing_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_key\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtuple\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numpy_indices\u001b[38;5;241m.\u001b[39mtuple:\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# index the loaded np.ndarray\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     result \u001b[38;5;241m=\u001b[39m NumpyIndexingAdapter(result)[numpy_indices]\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:113\u001b[0m, in \u001b[0;36mNetCDF4ArrayWrapper._getitem\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatastore\u001b[38;5;241m.\u001b[39mlock:\n\u001b[0;32m    112\u001b[0m         original_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_array(needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 113\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mgetitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;66;03m# Catch IndexError in netCDF4 and return a more informative\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# error message.  This is most often called when an unsorted\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# indexer is used before the data is loaded from disk.\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe indexing operation you are attempting to perform \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis not valid on netCDF4.Variable object. Try loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data into memory first by calling .load().\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    122\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Store true labels and predictions\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# Iterate through test dataset batches\n",
    "for i in range(len(ds_test)):  \n",
    "    X_batch, y_batch = ds_test[i]  # Load batch\n",
    "\n",
    "    # Extract and stack only relevant features\n",
    "    X_batch = np.stack([X_batch[k] for k in ALL_VARIABLES], axis=-1)\n",
    "\n",
    "    # Flatten spatial dimensions (same as training)\n",
    "    X_batch = X_batch.reshape(X_batch.shape[0], -1)\n",
    "\n",
    "    # Handle NaN values (replace with zero)\n",
    "    X_batch = np.nan_to_num(X_batch, nan=0.0)\n",
    "\n",
    "    # Predict batch labels\n",
    "    preds = classifier.predict(X_batch)\n",
    "\n",
    "    # Store true labels and predictions\n",
    "    y_true.extend(y_batch)\n",
    "    y_pred.extend(preds)\n",
    "\n",
    "    print(f\"Batch {i} prediction done\")\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 Metrics:\n",
      " - Class Balance: Counter({0: 122, 1: 6})\n",
      " - Accuracy: 0.7422\n",
      " - Precision: 0.0000\n",
      " - Recall: 0.0000\n",
      " - AUC-ROC: 0.3893\n",
      " - PR AUC: 0.0469\n",
      "----------------------------------------\n",
      "Batch 1 Metrics:\n",
      " - Class Balance: Counter({0: 116, 1: 12})\n",
      " - Accuracy: 0.7422\n",
      " - Precision: 0.1379\n",
      " - Recall: 0.3333\n",
      " - AUC-ROC: 0.5589\n",
      " - PR AUC: 0.1085\n",
      "----------------------------------------\n",
      "Batch 2 Metrics:\n",
      " - Class Balance: Counter({0: 123, 1: 5})\n",
      " - Accuracy: 0.7891\n",
      " - Precision: 0.0769\n",
      " - Recall: 0.4000\n",
      " - AUC-ROC: 0.6024\n",
      " - PR AUC: 0.0542\n",
      "----------------------------------------\n",
      "Batch 3 Metrics:\n",
      " - Class Balance: Counter({0: 123, 1: 5})\n",
      " - Accuracy: 0.7422\n",
      " - Precision: 0.0333\n",
      " - Recall: 0.2000\n",
      " - AUC-ROC: 0.4821\n",
      " - PR AUC: 0.0379\n",
      "----------------------------------------\n",
      "Batch 4 Metrics:\n",
      " - Class Balance: Counter({0: 118, 1: 10})\n",
      " - Accuracy: 0.7734\n",
      " - Precision: 0.1481\n",
      " - Recall: 0.4000\n",
      " - AUC-ROC: 0.6025\n",
      " - PR AUC: 0.1061\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Iterate through test dataset batches\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ds_test)):  \n\u001b[1;32m---> 11\u001b[0m     X_batch, y_batch \u001b[38;5;241m=\u001b[39m \u001b[43mds_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Load batch\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Extract and stack only relevant features\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     X_batch \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack([X_batch[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m ALL_VARIABLES], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\tornet\\data\\keras\\loader.py:108\u001b[0m, in \u001b[0;36mKerasDataLoader.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    106\u001b[0m element_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files_batch:\n\u001b[1;32m--> 108\u001b[0m     element_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mALL_VARIABLES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtilt_last\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtilt_last\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# Transforms\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m element_list:\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\tornet\\data\\loader.py:59\u001b[0m, in \u001b[0;36mread_file\u001b[1;34m(f, variables, n_frames, tilt_last)\u001b[0m\n\u001b[0;32m     57\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mint\u001b[39m(ds\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_id\u001b[39m\u001b[38;5;124m'\u001b[39m])],dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64)\n\u001b[0;32m     58\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mef_number\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mint\u001b[39m(ds\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mef_number\u001b[39m\u001b[38;5;124m'\u001b[39m])],dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64)\n\u001b[1;32m---> 59\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maz_lower\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mazimuth_limits\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     60\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maz_upper\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mazimuth_limits\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m1\u001b[39m:]) \n\u001b[0;32m     61\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrng_lower\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrange_limits\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\core\\dataarray.py:733\u001b[0m, in \u001b[0;36mDataArray.values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m    726\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;124;03m    The array's data as a numpy.ndarray.\u001b[39;00m\n\u001b[0;32m    728\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;124;03m    type does not support coercion like this (e.g. cupy).\u001b[39;00m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\core\\variable.py:614\u001b[0m, in \u001b[0;36mVariable.values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    613\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The variable's data as a numpy.ndarray\"\"\"\u001b[39;00m\n\u001b[1;32m--> 614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_as_array_or_item\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\core\\variable.py:314\u001b[0m, in \u001b[0;36m_as_array_or_item\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_as_array_or_item\u001b[39m(data):\n\u001b[0;32m    301\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the given values as a numpy array, or as an individual item if\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;124;03m    it's a 0d datetime64 or timedelta64 array.\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;124;03m    TODO: remove this (replace with np.asarray) once these issues are fixed\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    316\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\core\\indexing.py:693\u001b[0m, in \u001b[0;36mMemoryCachedArray.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: np\u001b[38;5;241m.\u001b[39mtyping\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m--> 693\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\core\\indexing.py:696\u001b[0m, in \u001b[0;36mMemoryCachedArray.get_duck_array\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 696\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    697\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray\u001b[38;5;241m.\u001b[39mget_duck_array()\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\core\\indexing.py:690\u001b[0m, in \u001b[0;36mMemoryCachedArray._ensure_cached\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    689\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ensure_cached\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 690\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray \u001b[38;5;241m=\u001b[39m as_indexable(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\core\\indexing.py:664\u001b[0m, in \u001b[0;36mCopyOnWriteArray.get_duck_array\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 664\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\core\\indexing.py:551\u001b[0m, in \u001b[0;36mLazilyIndexedArray.get_duck_array\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 551\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;66;03m# self.array[self.key] is now a numpy array when\u001b[39;00m\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;66;03m# self.array is a BackendArray subclass\u001b[39;00m\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;66;03m# and self.key is BasicIndexer((slice(None, None, None),))\u001b[39;00m\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;66;03m# so we need the explicit check for ExplicitlyIndexed\u001b[39;00m\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, ExplicitlyIndexed):\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:100\u001b[0m, in \u001b[0;36mNetCDF4ArrayWrapper.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mindexing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplicit_indexing_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIndexingSupport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOUTER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\core\\indexing.py:858\u001b[0m, in \u001b[0;36mexplicit_indexing_adapter\u001b[1;34m(key, shape, indexing_support, raw_indexing_method)\u001b[0m\n\u001b[0;32m    836\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Support explicit indexing by delegating to a raw indexing method.\u001b[39;00m\n\u001b[0;32m    837\u001b[0m \n\u001b[0;32m    838\u001b[0m \u001b[38;5;124;03mOuter and/or vectorized indexers are supported by indexing a second time\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;124;03mIndexing result, in the form of a duck numpy-array.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    857\u001b[0m raw_key, numpy_indices \u001b[38;5;241m=\u001b[39m decompose_indexer(key, shape, indexing_support)\n\u001b[1;32m--> 858\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mraw_indexing_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_key\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtuple\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numpy_indices\u001b[38;5;241m.\u001b[39mtuple:\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# index the loaded np.ndarray\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     result \u001b[38;5;241m=\u001b[39m NumpyIndexingAdapter(result)[numpy_indices]\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:113\u001b[0m, in \u001b[0;36mNetCDF4ArrayWrapper._getitem\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatastore\u001b[38;5;241m.\u001b[39mlock:\n\u001b[0;32m    112\u001b[0m         original_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_array(needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 113\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mgetitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;66;03m# Catch IndexError in netCDF4 and return a more informative\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# error message.  This is most often called when an unsorted\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# indexer is used before the data is loaded from disk.\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe indexing operation you are attempting to perform \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis not valid on netCDF4.Variable object. Try loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data into memory first by calling .load().\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    122\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_score, recall_score, accuracy_score\n",
    "from collections import Counter\n",
    "\n",
    "# Store cumulative true labels and predictions\n",
    "y_true_cumulative = []\n",
    "y_pred_cumulative = []\n",
    "\n",
    "# Iterate through test dataset batches\n",
    "for i in range(len(ds_test)):  \n",
    "    X_batch, y_batch = ds_test[i]  # Load batch\n",
    "\n",
    "    # Extract and stack only relevant features\n",
    "    X_batch = np.stack([X_batch[k] for k in ALL_VARIABLES], axis=-1)\n",
    "\n",
    "    # Flatten spatial dimensions (same as training)\n",
    "    X_batch = X_batch.reshape(X_batch.shape[0], -1)\n",
    "\n",
    "    # Handle NaN values (replace with zero)\n",
    "    X_batch = np.nan_to_num(X_batch, nan=0.0)\n",
    "\n",
    "    # Predict batch labels\n",
    "    preds = classifier.predict(X_batch)\n",
    "\n",
    "    # Store true labels and predictions for the batch\n",
    "    y_true_batch = np.array(y_batch)\n",
    "    y_pred_batch = np.array(preds)\n",
    "\n",
    "    # If classifier supports probability outputs, use them for AUC calculations\n",
    "    if hasattr(classifier, \"predict_proba\"):\n",
    "        y_pred_proba_batch = classifier.predict_proba(X_batch)[:, 1]  # Probability of class 1\n",
    "    else:\n",
    "        y_pred_proba_batch = y_pred_batch  # Use binary predictions if proba isn't available\n",
    "\n",
    "    # Compute batch-wise metrics\n",
    "    accuracy_batch = accuracy_score(y_true_batch, y_pred_batch)\n",
    "    precision_batch = precision_score(y_true_batch, y_pred_batch, zero_division=0)\n",
    "    recall_batch = recall_score(y_true_batch, y_pred_batch, zero_division=0)\n",
    "    auc_batch = roc_auc_score(y_true_batch, y_pred_proba_batch) if len(np.unique(y_true_batch)) > 1 else float('nan')\n",
    "    pr_auc_batch = average_precision_score(y_true_batch, y_pred_proba_batch) if len(np.unique(y_true_batch)) > 1 else float('nan')\n",
    "\n",
    "    # Print batch metrics\n",
    "    print(f\"Batch {i} Metrics:\")\n",
    "    print(f\" - Class Balance: {Counter(y_true_batch)}\")\n",
    "    print(f\" - Accuracy: {accuracy_batch:.4f}\")\n",
    "    print(f\" - Precision: {precision_batch:.4f}\")\n",
    "    print(f\" - Recall: {recall_batch:.4f}\")\n",
    "    print(f\" - AUC-ROC: {auc_batch:.4f}\")\n",
    "    print(f\" - PR AUC: {pr_auc_batch:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Update cumulative storage\n",
    "    y_true_cumulative.extend(y_true_batch)\n",
    "    y_pred_cumulative.extend(y_pred_batch)\n",
    "\n",
    "# Convert cumulative lists to NumPy arrays\n",
    "y_true_cumulative = np.array(y_true_cumulative)\n",
    "y_pred_cumulative = np.array(y_pred_cumulative)\n",
    "\n",
    "# Compute cumulative metrics\n",
    "accuracy_cum = accuracy_score(y_true_cumulative, y_pred_cumulative)\n",
    "precision_cum = precision_score(y_true_cumulative, y_pred_cumulative, zero_division=0)\n",
    "recall_cum = recall_score(y_true_cumulative, y_pred_cumulative, zero_division=0)\n",
    "auc_cum = roc_auc_score(y_true_cumulative, y_pred_cumulative) if len(np.unique(y_true_cumulative)) > 1 else float('nan')\n",
    "pr_auc_cum = average_precision_score(y_true_cumulative, y_pred_cumulative) if len(np.unique(y_true_cumulative)) > 1 else float('nan')\n",
    "\n",
    "# Print cumulative metrics\n",
    "print(\"Cumulative Metrics:\")\n",
    "print(f\" - Accuracy: {accuracy_cum:.4f}\")\n",
    "print(f\" - Precision: {precision_cum:.4f}\")\n",
    "print(f\" - Recall: {recall_cum:.4f}\")\n",
    "print(f\" - PR AUC: {pr_auc_cum:.4f}\")\n",
    "print(\"=\" * 40)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tornet-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
