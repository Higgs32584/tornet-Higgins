{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "936b2c4e-ddf7-4e02-88e3-e280af0f2621",
   "metadata": {},
   "source": [
    "# Training a simple CNN model in Tensorflow for Tornado Detection\n",
    "\n",
    "This notebook steps through how to train a simple CNN model using a subset of TorNet.\n",
    "\n",
    "This will not produce a model with any skill, but simply provides a working end-to-end example of how to set up a data loader, build, and fit a model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "392369dc-ade9-4d34-8a5f-9d7d7d24a69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Uncomment if tornet isn't installed in your environment or in your path already\n",
    "#sys.path.append('../')  \n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tornet.data.tf.loader import create_tf_dataset \n",
    "from tornet.data.constants import ALL_VARIABLES\n",
    "import tornet.data.preprocess as pp\n",
    "from tornet.data import preprocess as tfpp\n",
    "import keras\n",
    "from tornet.data.constants import CHANNEL_MIN_MAX\n",
    "import tornet.metrics.keras.metrics as km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "312868ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_region(row):\n",
    "    tornado_alley_bounds = {\n",
    "    'lat_min': 32.0,\n",
    "    'lat_max': 43.0,\n",
    "    'lon_min': -102.0,\n",
    "    'lon_max': -94.0\n",
    "}\n",
    "\n",
    "    dixie_alley_bounds = {\n",
    "        'lat_min': 30.0,\n",
    "        'lat_max': 38.0,\n",
    "        'lon_min': -94.0,\n",
    "        'lon_max': -83.0\n",
    "    }\n",
    "    if (\n",
    "        tornado_alley_bounds['lat_min'] <= row['lat'] <= tornado_alley_bounds['lat_max'] and\n",
    "        tornado_alley_bounds['lon_min'] <= row['lon'] <= tornado_alley_bounds['lon_max']\n",
    "    ):\n",
    "        return 2  # Tornado Alley\n",
    "    elif (\n",
    "        dixie_alley_bounds['lat_min'] <= row['lat'] <= dixie_alley_bounds['lat_max'] and\n",
    "        dixie_alley_bounds['lon_min'] <= row['lon'] <= dixie_alley_bounds['lon_max']\n",
    "    ):\n",
    "        return 1  # Dixie Alley\n",
    "    else:\n",
    "        return 0  # None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "188f31eb-e051-4d5f-880d-7ef0a8eddcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_dataset(data_root, data_type, years, variables, random_state=1234):\n",
    "    \"\"\"\n",
    "    Load and prepare the dataset from the given catalog and file list.\n",
    "\n",
    "    Args:\n",
    "        data_root (str): Path to the root directory containing the catalog and data files.\n",
    "        data_type (str): Type of data to load (e.g., 'train' or 'test').\n",
    "        years (list): List of years to filter the catalog.\n",
    "        variables (list): List of variables for the dataset.\n",
    "        random_state (int): Seed for shuffling.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: TensorFlow dataset prepared from the file list.\n",
    "    \"\"\"\n",
    "    catalog_path = os.path.join(data_root, 'catalog.csv')\n",
    "    if not os.path.exists(catalog_path):\n",
    "        raise RuntimeError(f'Unable to find catalog.csv at {data_root}')\n",
    "\n",
    "    catalog = pd.read_csv(catalog_path, parse_dates=['start_time', 'end_time'])\n",
    "    catalog = catalog[catalog['type'] == data_type]\n",
    "    catalog = catalog[catalog['category'] != \"NUL\"]\n",
    "    catalog = catalog[catalog.start_time.dt.year.isin(years)]\n",
    "    catalog['region'] = catalog.apply(classify_region, axis=1)\n",
    "    # catalog = catalog[catalog['region'] == 0]\n",
    "    catalog = catalog.sample(frac=1, random_state=random_state)\n",
    "\n",
    "    file_list = [os.path.join(data_root, f) for f in catalog.filename]\n",
    "    return create_tf_dataset(file_list, variables=variables)\n",
    "\n",
    "# Location of tornet\n",
    "data_root = \"C:/Users/mjhig/tornet_2013\"\n",
    "\n",
    "# # Get training dataset\n",
    "years = [2013]\n",
    "#, 2014, 2015, 2016, 2017, 2018]\n",
    "ds = load_dataset(data_root, data_type='train', years=years, variables=ALL_VARIABLES)\n",
    "ds = ds.map(lambda d: pp.add_coordinates(d, include_az=False, backend=tf))\n",
    "ds = ds.map(pp.remove_time_dim)\n",
    "ds = ds.map(tfpp.split_x_y)\n",
    "ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "ds = ds.batch(32)\n",
    "# Get test dataset\n",
    "ds_test = load_dataset(data_root, data_type='test', years=years, variables=ALL_VARIABLES)\n",
    "# preprocess\n",
    "ds_test = ds_test.map(lambda d: pp.add_coordinates(d,include_az=False,backend=tf))\n",
    "ds_test = ds_test.map(pp.remove_time_dim)\n",
    "ds_test = ds_test.map(tfpp.split_x_y)\n",
    "ds_test = ds_test.prefetch(tf.data.AUTOTUNE)\n",
    "ds_test = ds_test.batch(32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c668b9e",
   "metadata": {},
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "def load_dataset(data_root, data_type, years, variables, random_state=1234):\n",
    "    \"\"\"\n",
    "    Load and prepare the dataset from the given catalog and file list.\n",
    "\n",
    "    Args:\n",
    "        data_root (str): Path to the root directory containing the catalog and data files.\n",
    "        data_type (str): Type of data to load (e.g., 'train' or 'test').\n",
    "        years (list): List of years to filter the catalog.\n",
    "        variables (list): List of variables for the dataset.\n",
    "        random_state (int): Seed for shuffling.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: TensorFlow dataset prepared from the file list.\n",
    "    \"\"\"\n",
    "    catalog_path = os.path.join(data_root, 'catalog.csv')\n",
    "    if not os.path.exists(catalog_path):\n",
    "        raise RuntimeError(f'Unable to find catalog.csv at {data_root}')\n",
    "\n",
    "    catalog = pd.read_csv(catalog_path, parse_dates=['start_time', 'end_time'])\n",
    "    catalog = catalog[catalog['category'] == 'TOR']\n",
    "    catalog = catalog[catalog['type'] == data_type]\n",
    "    catalog = catalog[catalog.start_time.dt.year.isin(years)]\n",
    "    catalog['region'] = catalog.apply(classify_region, axis=1)\n",
    "    catalog=catalog[catalog.region != 0]\n",
    "    catalog = catalog.sample(frac=1, random_state=random_state)\n",
    "\n",
    "    file_list = [os.path.join(data_root, f) for f in catalog.filename]\n",
    "    return create_tf_dataset(file_list, variables=variables)\n",
    "\n",
    "\n",
    "def new_x_y(d):\n",
    "    \"\"\"\n",
    "    Splits dict into X,y, where y are tornado labels\n",
    "    \"\"\"\n",
    "    print(d['coordinates'])\n",
    "    y=classify_region(d)\n",
    "    return d,y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d4ead5",
   "metadata": {},
   "source": [
    "# Location of tornet\n",
    "data_root = \"C:/Users/mjhig/tornet_2013\"\n",
    "\n",
    "# Define years and variables\n",
    "years = [2013, 2014, 2015]\n",
    "\n",
    "# Get training dataset\n",
    "ds = load_dataset(data_root, data_type='train', years=years, variables=ALL_VARIABLES)\n",
    "ds = ds.map(lambda d: pp.add_coordinates(d, include_az=False, backend=tf))\n",
    "ds = ds.map(pp.remove_time_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20efc4a",
   "metadata": {},
   "source": [
    "def classify_region(row):\n",
    "    tornado_alley_bounds = {\n",
    "    'lat_min': 32.0,\n",
    "    'lat_max': 43.0,\n",
    "    'lon_min': -102.0,\n",
    "    'lon_max': -94.0\n",
    "}\n",
    "\n",
    "    dixie_alley_bounds = {\n",
    "        'lat_min': 30.0,\n",
    "        'lat_max': 38.0,\n",
    "        'lon_min': -94.0,\n",
    "        'lon_max': -83.0\n",
    "    }\n",
    "    if (\n",
    "        tornado_alley_bounds['lat_min'] <= row['lat'] <= tornado_alley_bounds['lat_max'] and\n",
    "        tornado_alley_bounds['lon_min'] <= row['lon'] <= tornado_alley_bounds['lon_max']\n",
    "    ):\n",
    "        return 2  # Tornado Alley\n",
    "    elif (\n",
    "        dixie_alley_bounds['lat_min'] <= row['lat'] <= dixie_alley_bounds['lat_max'] and\n",
    "        dixie_alley_bounds['lon_min'] <= row['lon'] <= dixie_alley_bounds['lon_max']\n",
    "    ):\n",
    "        return 1  # Dixie Alley\n",
    "    else:\n",
    "        return 0  # None\n",
    "\n",
    "def new_x_y(d):\n",
    "    \"\"\"\n",
    "    Splits dict into X,y, where y are tornado labels\n",
    "    \"\"\"\n",
    "    print(d['coordinates'])\n",
    "\n",
    "ds = ds.map(new_x_y)\n",
    "# ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "# ds = ds.batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2ddca1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Get test dataset\n",
    "ds_test = load_dataset(data_root, data_type='test', years=years, variables=ALL_VARIABLES)\n",
    "ds_test = ds_test.map(lambda d: pp.add_coordinates(d,include_az=False,backend=tf))\n",
    "ds_test = ds_test.map(pp.remove_time_dim)\n",
    "ds_test = ds_test.map(tfpp.split_x_y)\n",
    "ds_test = filter_positive_tornado_cases(ds_test)  # Filter by positive tornado cases\n",
    "ds_test = set_target_to_region(ds_test)          # Set the target to 'region'\n",
    "ds_test = ds_test.prefetch(tf.data.AUTOTUNE)\n",
    "ds_test = ds_test.batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3521d09b",
   "metadata": {},
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "def create_model(architecture):\n",
    "    \"\"\"Create a CNN model with different architectures.\"\"\"\n",
    "    inputs = {v: keras.Input(shape=(120, 240, 2), name=v) for v in ALL_VARIABLES}\n",
    "    norm_layers = []\n",
    "    for v in ALL_VARIABLES:\n",
    "        min_max = np.array(CHANNEL_MIN_MAX[v])\n",
    "        var = ((min_max[1] - min_max[0]) / 2) ** 2\n",
    "        var = np.array(2 * [var])\n",
    "        offset = (min_max[0] + min_max[1]) / 2\n",
    "        offset = np.array(2 * [offset])\n",
    "        norm_layers.append(\n",
    "            keras.layers.Normalization(mean=offset, variance=var, name=f'Normalized_{v}')\n",
    "        )\n",
    "    x = keras.layers.Concatenate(axis=-1, name='Concatenate1')(\n",
    "        [l(inputs[v]) for l, v in zip(norm_layers, ALL_VARIABLES)]\n",
    "    )\n",
    "    x = keras.layers.Lambda(lambda x: tf.where(tf.math.is_nan(x), -3.0, x), name='ReplaceNan')(x)\n",
    "    \n",
    "    if architecture == \"baseline\":\n",
    "        x = keras.layers.Conv2D(32, 3, padding='same', activation='relu')(x)\n",
    "        x = keras.layers.Conv2D(1, 1, padding='same', activation='relu', name='TornadoLikelihood')(x)\n",
    "    elif architecture == \"deeper\":\n",
    "        x = keras.layers.Conv2D(32, 3, padding='same', activation='relu')(x)\n",
    "        x = keras.layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "        x = keras.layers.Conv2D(1, 1, padding='same', activation='relu', name='TornadoLikelihood')(x)\n",
    "    elif architecture == \"residual\":\n",
    "        x = keras.layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "        x= keras.layers.Conv2D(32, 3, padding='same', activation='relu')(x)\n",
    "        x = keras.layers.Conv2D(1, 1, padding='same', activation='relu', name='TornadoLikelihood')(x)\n",
    "    elif architecture == \"wide\":\n",
    "        x = keras.layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "        x = keras.layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "        x = keras.layers.Conv2D(1, 1, padding='same', activation='relu', name='TornadoLikelihood')(x)\n",
    "    elif architecture == \"dropout\":\n",
    "        x = keras.layers.Conv2D(32, 3, padding='same', activation='relu')(x)\n",
    "        x = keras.layers.Dropout(0.5)(x)\n",
    "        x = keras.layers.Conv2D(1, 1, padding='same', activation='relu', name='TornadoLikelihood')(x)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown architecture type\")\n",
    "    \n",
    "    y = keras.layers.GlobalMaxPool2D(name='GlobalMaxPool')(x)\n",
    "    return keras.Model(inputs=inputs, outputs=y, name=f'TornadoDetector_{architecture}')\n",
    "\n",
    "# Train and evaluate different models\n",
    "architectures = [\"baseline\", \"deeper\", \"residual\", \"wide\", \"dropout\"]\n",
    "\n",
    "model = create_model(\"baseline\")\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),loss=keras.losses.BinaryCrossentropy(from_logits=True))\n",
    "model.fit(ds, epochs=4, steps_per_epoch=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6cccc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DBZ': TensorShape([32, 1, 120, 240, 2]), 'VEL': TensorShape([32, 1, 120, 240, 2]), 'KDP': TensorShape([32, 1, 120, 240, 2]), 'RHOHV': TensorShape([32, 1, 120, 240, 2]), 'ZDR': TensorShape([32, 1, 120, 240, 2]), 'WIDTH': TensorShape([32, 1, 120, 240, 2]), 'range_folded_mask': TensorShape([32, 1, 120, 240, 2]), 'label': TensorShape([32, 1]), 'category': TensorShape([32, 1]), 'event_id': TensorShape([32, 1]), 'ef_number': TensorShape([32, 1]), 'az_lower': TensorShape([32, 1]), 'az_upper': TensorShape([32, 1]), 'rng_lower': TensorShape([32, 1]), 'rng_upper': TensorShape([32, 1]), 'time': TensorShape([32, 1]), 'tornado_start_time': TensorShape([32, 1]), 'tornado_end_time': TensorShape([32, 1])}\n"
     ]
    }
   ],
   "source": [
    "for x, y in ds.take(1):\n",
    "    print({k: v.shape for k, v in x.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aed74de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training dataset\n",
    "years = [2013, 2014, 2015, 2016, 2017, 2018]\n",
    "ds = load_dataset(data_root, data_type='train', years=years, variables=ALL_VARIABLES)\n",
    "ds_test = load_dataset(data_root, data_type='test', years=years, variables=ALL_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1b3d982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset preprocessing skipped.\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"TornadoDetector_improved\" is incompatible with the layer: expected shape=(None, 120, 240, 2), found shape=(None, 1, 120, 240, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 109\u001b[0m\n\u001b[0;32m    102\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m    103\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.01\u001b[39m),\n\u001b[0;32m    104\u001b[0m     loss\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mBinaryCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m    105\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[AUC(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAUC\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m    106\u001b[0m )\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestore_best_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m history_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(history\u001b[38;5;241m.\u001b[39mhistory)\n\u001b[0;32m    115\u001b[0m history_df\u001b[38;5;241m.\u001b[39mplot()\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\site-packages\\keras\\src\\layers\\input_spec.py:245\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec_dim \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[1;32m--> 245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    246\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    250\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"TornadoDetector_improved\" is incompatible with the layer: expected shape=(None, 120, 240, 2), found shape=(None, 1, 120, 240, 2)"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv2D, Dropout, GlobalAveragePooling2D, Dense, BatchNormalization\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "def add_derived_features(inputs):\n",
    "    \"\"\"Add only ZDR/KDP ratio as a derived feature.\"\"\"\n",
    "    derived_inputs = {}\n",
    "    if \"ZDR\" in inputs and \"KDP\" in inputs:\n",
    "        # Compute ZDR/KDP ratio\n",
    "        zdr_kdp_ratio = Lambda(\n",
    "            lambda x: tf.math.divide_no_nan(x[0], x[1] + 1e-6), name=\"zdr_kdp_ratio\"\n",
    "        )([inputs[\"ZDR\"], inputs[\"KDP\"]])\n",
    "        derived_inputs[\"zdr_kdp_ratio\"] = zdr_kdp_ratio\n",
    "\n",
    "    inputs.update(derived_inputs)\n",
    "    return inputs\n",
    "\n",
    "def create_model_improved(architecture):\n",
    "    \"\"\"Create a model using six base radar images and ZDR/KDP ratio.\"\"\"\n",
    "    inputs = {\n",
    "        \"DBZ\": Input(shape=(120, 240, 2), name=\"DBZ\"),\n",
    "        \"VEL\": Input(shape=(120, 240, 2), name=\"VEL\"),\n",
    "        \"KDP\": Input(shape=(120, 240, 2), name=\"KDP\"),\n",
    "        \"RHOHV\": Input(shape=(120, 240, 2), name=\"RHOHV\"),\n",
    "        \"ZDR\": Input(shape=(120, 240, 2), name=\"ZDR\"),\n",
    "        \"WIDTH\": Input(shape=(120, 240, 2), name=\"WIDTH\"),\n",
    "    }\n",
    "\n",
    "\n",
    "    # Normalize inputs\n",
    "    norm_layers = []\n",
    "    for v in inputs:\n",
    "        min_max = np.array(CHANNEL_MIN_MAX.get(v, [0, 1]))\n",
    "        var = ((min_max[1] - min_max[0]) / 2) ** 2\n",
    "        var = np.array(2 * [var])\n",
    "        offset = (min_max[0] + min_max[1]) / 2\n",
    "        offset = np.array(2 * [offset])\n",
    "        norm_layers.append(\n",
    "            keras.layers.Normalization(mean=offset, variance=var, name=f'Normalized_{v}')\n",
    "        )\n",
    "\n",
    "    # Concatenate normalized inputs\n",
    "    x = keras.layers.Concatenate(axis=-1, name='Concatenate1')(\n",
    "        [l(inputs[v]) for l, v in zip(norm_layers, inputs.keys())]\n",
    "    )\n",
    "    x = keras.layers.Lambda(lambda x: tf.where(tf.math.is_nan(x), -3.0, x), name='ReplaceNan')(x)\n",
    "\n",
    "    # Architecture-specific logic\n",
    "    if architecture == \"improved\":\n",
    "        x = Conv2D(32, 3, padding='same', activation='relu', kernel_regularizer='l2')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = Conv2D(64, 3, padding='same', activation='relu', kernel_regularizer='l2')(x)\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dense(1, activation='sigmoid', name='TornadoLikelihood')(x)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown architecture type\")\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=x, name=f'TornadoDetector_{architecture}')\n",
    "\n",
    "def preprocess_with_derived_features(inputs, labels):\n",
    "    \"\"\"Add ZDR/KDP ratio during preprocessing.\"\"\"\n",
    "    feature_inputs = {\n",
    "        \"DBZ\": inputs[\"DBZ\"],\n",
    "        \"VEL\": inputs[\"VEL\"],\n",
    "        \"KDP\": inputs[\"KDP\"],\n",
    "        \"RHOHV\": inputs[\"RHOHV\"],\n",
    "        \"ZDR\": inputs[\"ZDR\"],\n",
    "        \"WIDTH\": inputs[\"WIDTH\"],\n",
    "    }\n",
    "\n",
    "    # Add ZDR/KDP ratio with the correct tensor name\n",
    "    if \"ZDR\" in inputs and \"KDP\" in inputs:\n",
    "        zdr_kdp_ratio = tf.math.divide_no_nan(inputs[\"ZDR\"], inputs[\"KDP\"] + 1e-6)\n",
    "        feature_inputs[\"zdr_kdp_ratio\"] = tf.identity(zdr_kdp_ratio, name=\"zdr_kdp_ratio\")  # Explicitly name the tensor\n",
    "\n",
    "    return feature_inputs, labels\n",
    "\n",
    "\n",
    "# Define the batch size\n",
    "BATCH_SIZE = 32\n",
    "data_root = \"C:/Users/mjhig/tornet_2013\"\n",
    "\n",
    "# Preprocess datasets\n",
    "try:\n",
    "    ds = (\n",
    "        ds\n",
    "        .map(tfpp.split_x_y)\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "        .batch(BATCH_SIZE)\n",
    "    )\n",
    "except:\n",
    "    print(\"Dataset preprocessing skipped.\")\n",
    "\n",
    "\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "# Create the model\n",
    "model = create_model_improved(\"improved\")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=.01),\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    metrics=[AUC(name=\"AUC\")]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(ds,epochs=3, callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True),\n",
    "        keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2),\n",
    "    ]\n",
    ")\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.plot()\n",
    "# # Evaluate the model\n",
    "# results = model.evaluate(ds_test)\n",
    "# print(f\"Test results: {results}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecf232c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input keys: dict_keys(['DBZ_gradients', 'DBZ', 'VEL_gradients', 'VEL', 'KDP_gradients', 'KDP', 'RHOHV_gradients', 'RHOHV', 'ZDR_gradients', 'ZDR', 'WIDTH_gradients', 'WIDTH', 'zdr_kdp_ratio'])\n"
     ]
    }
   ],
   "source": [
    "for batch in ds.take(1):\n",
    "    inputs, labels = batch\n",
    "    print(\"Input keys:\", inputs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333e21b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "import tornet.metrics.keras.metrics as km\n",
    "metrics = [keras.metrics.AUC(from_logits=True,name='AUC'),\n",
    "           km.BinaryAccuracy(from_logits=True,name='BinaryAccuracy'), \n",
    "           ]\n",
    "model.compile(loss=keras.losses.BinaryCrossentropy(from_logits=True),metrics=metrics)\n",
    "\n",
    "# steps=10 for demo purposes\n",
    "model.evaluate(ds_test,steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf7832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_gradcam_heatmap_with_input_gradients(model_inputs, model, last_conv_layer_name, pred_index=None):\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs],\n",
    "        [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(model_inputs)  # Watch the inputs to compute gradients\n",
    "        conv_outputs, predictions = grad_model(model_inputs)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(predictions[0])\n",
    "        class_channel = predictions[:, pred_index]\n",
    "\n",
    "    # Gradients with respect to the inputs\n",
    "    input_grads = tape.gradient(class_channel, model_inputs)\n",
    "\n",
    "    # Gradients with respect to the last conv layer\n",
    "    conv_grads = tape.gradient(class_channel, conv_outputs)\n",
    "    pooled_grads = tf.reduce_mean(conv_grads, axis=(0, 1, 2))  # Average gradients over width and height\n",
    "    #pooled_grads=conv_grads\n",
    "    heatmap = tf.reduce_sum(tf.multiply(pooled_grads, conv_outputs), axis=-1)  # Sum across channels\n",
    "\n",
    "    heatmap = np.maximum(heatmap, 0)  # ReLU to remove negative values\n",
    "    if tf.reduce_max(heatmap) > 0:\n",
    "        heatmap /= tf.reduce_max(heatmap)  # Normalize heatmap to [0, 1]\n",
    "\n",
    "    return np.array(heatmap), input_grads\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Unpack the dataset\n",
    "for sample in ds_test.take(1):  # Take a single batch from the dataset\n",
    "    inputs, _ = sample  # Assuming the dataset is structured as (features, labels)\n",
    "    break\n",
    "\n",
    "# Prepare the list of inputs for the model\n",
    "model_inputs = [\n",
    "    inputs['DBZ'], \n",
    "    inputs['VEL'], \n",
    "    inputs['KDP'], \n",
    "    inputs['RHOHV'], \n",
    "    inputs['ZDR'], \n",
    "    inputs['WIDTH']\n",
    "]\n",
    "\n",
    "# Generate heatmap and input gradients\n",
    "heatmap, input_grads = make_gradcam_heatmap_with_input_gradients(model_inputs, model, 'TornadoLikelihood')\n",
    "\n",
    "# Calculate the importance of each input\n",
    "input_importance = [tf.reduce_mean(tf.abs(grad)).numpy() for grad in input_grads]\n",
    "\n",
    "# Map input names to their importance\n",
    "input_names = ['DBZ', 'VEL', 'KDP', 'RHOHV', 'ZDR', 'WIDTH']\n",
    "input_importance_dict = dict(zip(input_names, input_importance))\n",
    "\n",
    "# Print or plot the importance of each input\n",
    "print(\"Input importance:\", input_importance_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d72ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_gradcam_heatmap_with_input_gradients(model_inputs, model, last_conv_layer_name, pred_index=None):\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs],\n",
    "        [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(model_inputs)  # Watch the inputs to compute gradients\n",
    "        conv_outputs, predictions = grad_model(model_inputs)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(predictions[0])\n",
    "        class_channel = predictions[:, pred_index]\n",
    "\n",
    "    # Gradients with respect to the inputs\n",
    "    input_grads = tape.gradient(class_channel, model_inputs)\n",
    "\n",
    "    # Gradients with respect to the last conv layer\n",
    "    conv_grads = tape.gradient(class_channel, conv_outputs)\n",
    "    #pooled_grads = tf.reduce_mean(conv_grads, axis=(0, 1, 2))  # Average gradients over width and height\n",
    "    pooled_grads=conv_grads\n",
    "    heatmap = tf.reduce_sum(tf.multiply(pooled_grads, conv_outputs), axis=-1)  # Sum across channels\n",
    "\n",
    "    heatmap = np.maximum(heatmap, 0)  # ReLU to remove negative values\n",
    "    if tf.reduce_max(heatmap) > 0:\n",
    "        heatmap /= tf.reduce_max(heatmap)  # Normalize heatmap to [0, 1]\n",
    "\n",
    "    return np.array(heatmap), input_grads\n",
    "\n",
    "# Unpack the dataset\n",
    "for sample in ds_test.take(1):  # Take a single batch from the dataset\n",
    "    inputs, y = sample  # Assuming the dataset is structured as (features, labels)\n",
    "    break\n",
    "\n",
    "# Prepare the list of inputs for the model\n",
    "model_inputs = [\n",
    "    inputs['DBZ'], \n",
    "    inputs['VEL'], \n",
    "    inputs['KDP'], \n",
    "    inputs['RHOHV'], \n",
    "    inputs['ZDR'], \n",
    "    inputs['WIDTH']\n",
    "]\n",
    "\n",
    "# Generate heatmap and input gradients\n",
    "heatmap, input_grads = make_gradcam_heatmap_with_input_gradients(model_inputs, model, 'TornadoLikelihood')\n",
    "\n",
    "# Calculate the importance of each input\n",
    "input_importance = [tf.reduce_mean(tf.abs(grad)).numpy() for grad in input_grads]\n",
    "\n",
    "# Map input names to their importance\n",
    "input_names = ['DBZ', 'VEL', 'KDP', 'RHOHV', 'ZDR', 'WIDTH']\n",
    "radar_cmap_mapping = {\n",
    "    \"DBZ\": \"viridis\",  # Reflectivity\n",
    "    \"VEL\": \"coolwarm\",  # Velocity\n",
    "    \"KDP\": \"Greens\",    # Specific Differential Phase\n",
    "    \"RHOHV\": \"hot\",     # Correlation Coefficient\n",
    "    \"ZDR\": \"coolwarm\",  # Differential Reflectivity\n",
    "    \"WIDTH\": \"inferno\"  # Spectrum Width\n",
    "}\n",
    "input_importance_dict = dict(zip(input_names, input_importance))\n",
    "\n",
    "print(\"Input importance:\", input_importance_dict)\n",
    "\n",
    "# Analyze and plot each input slice\n",
    "for i, grad in enumerate(input_grads):\n",
    "    input_name = input_names[i]\n",
    "\n",
    "    # Compute mean absolute gradient for each spatial location\n",
    "    grad_abs_mean = tf.reduce_mean(tf.abs(grad), axis=-1).numpy()  # Shape: (batch_size, height, width)\n",
    "\n",
    "    # Select the most important regions for the first sample in the batch\n",
    "    important_regions = grad_abs_mean[0]  # First sample in batch\n",
    "    input_data = model_inputs[i][7]\n",
    "    print(y)\n",
    "    input_data=input_data[..., 0] \n",
    "    print(f\"Most important regions for {input_name}:\")\n",
    "    print(\"Shape of importance map:\", important_regions.shape)\n",
    "    print(input_data.shape)\n",
    "    # Visualize the input data and importance map side by side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6), edgecolor='k')\n",
    "    fig.suptitle(f\"Input and Importance Map for {input_name}\")\n",
    "\n",
    "    # Plot the original input\n",
    "    input_name = input_names[i]\n",
    "    axes[0].imshow(input_data,cmap=radar_cmap_mapping[input_name])\n",
    "    axes[0].set_title(f\"Original {input_name}\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Plot the importance map\n",
    "    im = axes[1].imshow(important_regions, cmap=\"hot\")\n",
    "    axes[1].set_title(f\"Importance Map for {input_name}\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    # Add colorbar to the importance map\n",
    "    fig.colorbar(im, ax=axes[1], label=\"Importance\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c159b9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tornet.display.display import plot_radar\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_gradcam_heatmap_with_input_gradients(model_inputs, model, last_conv_layer_name, pred_index=None):\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        [model.inputs],\n",
    "        [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(model_inputs)  # Watch the inputs to compute gradients\n",
    "        conv_outputs, predictions = grad_model(model_inputs)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(predictions[0])\n",
    "        class_channel = predictions[:, pred_index]\n",
    "\n",
    "    # Gradients with respect to the inputs\n",
    "    input_grads = tape.gradient(class_channel, model_inputs)\n",
    "\n",
    "    # Gradients with respect to the last conv layer\n",
    "    conv_grads = tape.gradient(class_channel, conv_outputs)\n",
    "    pooled_grads = tf.reduce_mean(conv_grads, axis=(0, 1, 2))  # Average gradients over width and height\n",
    "    heatmap = tf.reduce_sum(tf.multiply(pooled_grads, conv_outputs), axis=-1)  # Sum across channels\n",
    "\n",
    "    heatmap = np.maximum(heatmap, 0)  # ReLU to remove negative values\n",
    "    if tf.reduce_max(heatmap) > 0:\n",
    "        heatmap /= tf.reduce_max(heatmap)  # Normalize heatmap to [0, 1]\n",
    "\n",
    "    return np.array(heatmap), input_grads\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Unpack the dataset\n",
    "for sample in ds_test.take(1):  # Take a single batch from the dataset\n",
    "    inputs, _ = sample  # Assuming the dataset is structured as (features, labels)\n",
    "    break\n",
    "\n",
    "# Prepare the list of inputs for the model\n",
    "model_inputs = [\n",
    "    inputs['DBZ'], \n",
    "    inputs['VEL'], \n",
    "    inputs['KDP'], \n",
    "    inputs['RHOHV'], \n",
    "    inputs['ZDR'], \n",
    "    inputs['WIDTH']\n",
    "]\n",
    "\n",
    "# Generate heatmap and input gradients\n",
    "heatmap, input_grads = make_gradcam_heatmap_with_input_gradients(model_inputs, model, 'TornadoLikelihood')\n",
    "\n",
    "# Calculate the importance of each input\n",
    "input_importance = [tf.reduce_mean(tf.abs(grad)).numpy() for grad in input_grads]\n",
    "\n",
    "# Map input names to their importance\n",
    "input_names = ['DBZ', 'VEL', 'KDP', 'RHOHV', 'ZDR', 'WIDTH']\n",
    "input_importance_dict = dict(zip(input_names, input_importance))\n",
    "\n",
    "# Print or plot the importance of each input\n",
    "print(\"Input importance:\", input_importance_dict)\n",
    "\n",
    "\n",
    "# Analyze each input slice\n",
    "for i, grad in enumerate(input_grads):\n",
    "    input_name = input_names[i]\n",
    "\n",
    "    # Compute mean absolute gradient for each spatial location\n",
    "    grad_abs_mean = tf.reduce_mean(tf.abs(grad), axis=-1).numpy()  # Shape: (batch_size, height, width)\n",
    "\n",
    "    # Select the most important regions for the first sample in the batch\n",
    "    important_regions = grad_abs_mean[0]  # First sample in batch\n",
    "\n",
    "    print(f\"Most important regions for {input_name}:\")\n",
    "    print(\"Shape of importance map:\", important_regions.shape)\n",
    "\n",
    "    # Visualize the importance map\n",
    "    fig = plt.figure(figsize=(12,6),edgecolor='k')\n",
    "    plt.title(f\"Importance Map for {input_name}\")\n",
    "    #plot_radar(data=important_regions,fig=fig,channels=ALL_VARIABLES)\n",
    "    plt.imshow(important_regions, cmap=\"hot\")\n",
    "    plt.colorbar(label=\"Importance\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba29d75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to create derived features from radar variables\n",
    "def create_derived_features(inputs):\n",
    "    derived_features = []\n",
    "    for var_name, data in inputs.items():\n",
    "        # Skip non-radar variables\n",
    "        if var_name not in [\"DBZ\", \"VEL\", \"KDP\", \"RHOHV\", \"ZDR\", \"WIDTH\"]:\n",
    "            continue\n",
    "        \n",
    "        # Global statistics\n",
    "        global_mean = np.mean(data)\n",
    "        global_std = np.std(data)\n",
    "        global_max = np.max(data)\n",
    "        global_min = np.min(data)\n",
    "        \n",
    "        # Spatial gradients\n",
    "        dx = np.mean(np.gradient(data, axis=1))  # Vertical gradient\n",
    "        dy = np.mean(np.gradient(data, axis=2))  # Horizontal gradient\n",
    "        \n",
    "        # Add derived features\n",
    "        derived_features.extend([global_mean, global_std, global_max, global_min, dx, dy])\n",
    "        \n",
    "        # Custom meteorological feature: Example ZDR/KDP ratio\n",
    "        if var_name == \"ZDR\" and \"KDP\" in inputs:\n",
    "            zdr_kdp_ratio = np.mean(inputs[\"ZDR\"]) / (np.mean(inputs[\"KDP\"]) + 1e-6)\n",
    "            derived_features.append(zdr_kdp_ratio)\n",
    "    \n",
    "    return np.array(derived_features)\n",
    "\n",
    "# Generate features and labels from the dataset\n",
    "def generate_features_and_labels(dataset):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for batch in dataset:\n",
    "        batch_inputs, batch_labels = batch\n",
    "        for i in range(batch_inputs[\"DBZ\"].shape[0]):  # Iterate over each sample in the batch\n",
    "            sample_inputs = {k: v[i].numpy() for k, v in batch_inputs.items()}\n",
    "            sample_label = batch_labels[i].numpy()\n",
    "            \n",
    "            # Create derived features\n",
    "            derived_features = create_derived_features(sample_inputs)\n",
    "            features.append(derived_features)\n",
    "            labels.append(sample_label)\n",
    "    \n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Generate derived features and labels\n",
    "features, labels = generate_features_and_labels(ds_test)\n",
    "\n",
    "# Normalize features for further analysis\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest model to calculate feature importance\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Calculate feature importance\n",
    "feature_importance = rf_model.feature_importances_\n",
    "\n",
    "# # Calculate mutual information\n",
    "# mutual_info = mutual_info_classif(X_train, y_train, random_state=42)\n",
    "\n",
    "# # Create a DataFrame to rank features\n",
    "# num_features = features.shape[1]\n",
    "# feature_names = [f\"Feature_{i}\" for i in range(num_features)]\n",
    "# importance_df = pd.DataFrame({\n",
    "#     \"Feature\": feature_names,\n",
    "#     \"Importance\": feature_importance,\n",
    "#     \"Mutual Information\": mutual_info\n",
    "# })\n",
    "\n",
    "# # Sort features by importance\n",
    "# importance_df = importance_df.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# # Display the feature ranking\n",
    "# import ace_tools as tools; tools.display_dataframe_to_user(name=\"Feature Importance and Mutual Information\", dataframe=importance_df)\n",
    "\n",
    "# # Plot feature importance\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.barh(importance_df[\"Feature\"], importance_df[\"Importance\"])\n",
    "# plt.gca().invert_yaxis()\n",
    "# plt.title(\"Feature Importance\")\n",
    "# plt.xlabel(\"Importance\")\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tornet-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
