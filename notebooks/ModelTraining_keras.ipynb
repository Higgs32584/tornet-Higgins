{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "936b2c4e-ddf7-4e02-88e3-e280af0f2621",
   "metadata": {},
   "source": [
    "# Training a simple CNN model in `keras` (>3.0) for Tornado Detection\n",
    "\n",
    "This notebook steps through how to train a simple CNN model using a subset of TorNet.\n",
    "\n",
    "This will not produce a model with any skill, but simply provides a working end-to-end example of how to set up a data loader, build, and fit a model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b441b4a7-07b3-42f9-b7b2-0925136c37fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND']='tensorflow' # set to 'tensorflow', 'torch' or 'jax' (installs required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6117220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'data']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tornet as t\n",
    "dir(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "392369dc-ade9-4d34-8a5f-9d7d7d24a69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Uncomment if tornet isn't installed in your environment or in your path already\n",
    "sys.path.append('../')  \n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36fe2283",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tornet.data.tf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtornet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_tf_dataset \n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtornet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ALL_VARIABLES\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tornet.data.tf'"
     ]
    }
   ],
   "source": [
    "from tornet.data.tf.loader import create_tf_dataset \n",
    "from tornet.data.constants import ALL_VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "188f31eb-e051-4d5f-880d-7ef0a8eddcfd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'TORNET_ROOT'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# keras accepts most data loaders (tensorflow, torch).\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# A pure keras data loader, with necessary preprocessing steps for the cnn baseline, is provided\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtornet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasDataLoader\n\u001b[1;32m----> 4\u001b[0m data_root \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTORNET_ROOT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      5\u001b[0m ds \u001b[38;5;241m=\u001b[39m KerasDataLoader(data_root\u001b[38;5;241m=\u001b[39mdata_root,\n\u001b[0;32m      6\u001b[0m                      data_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m                      years\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2018\u001b[39m,\u001b[38;5;241m2019\u001b[39m],\n\u001b[0;32m      8\u001b[0m                      batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m, \n\u001b[0;32m      9\u001b[0m                      workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m     10\u001b[0m                      use_multiprocessing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\mjhig\\anaconda3\\envs\\tornet-torch\\lib\\os.py:680\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    677\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodekey(key)]\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;66;03m# raise KeyError with the original key value\u001b[39;00m\n\u001b[1;32m--> 680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodevalue(value)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'TORNET_ROOT'"
     ]
    }
   ],
   "source": [
    "# keras accepts most data loaders (tensorflow, torch).\n",
    "# A pure keras data loader, with necessary preprocessing steps for the cnn baseline, is provided\n",
    "from tornet.data.keras.loader import KerasDataLoader\n",
    "data_root = os.environ['TORNET_ROOT']\n",
    "ds = KerasDataLoader(data_root=data_root,\n",
    "                     data_type='train',\n",
    "                     years=[2018,2019],\n",
    "                     batch_size = 8, \n",
    "                     workers = 4,\n",
    "                     use_multiprocessing = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d24ab7-0c41-4f14-864a-58d9831a2311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple CNN model\n",
    "# This normalizes data, concatenates along channel, and applies a Conv2D\n",
    "from tornet.data.constants import CHANNEL_MIN_MAX\n",
    "from tornet.models.keras.layers import FillNaNs\n",
    "\n",
    "input_vars = ALL_VARIABLES # which variables to use\n",
    "\n",
    "# TF convention is B,L,W,H\n",
    "inputs = {v:keras.Input(shape=(120,240,2),name=v) for v in input_vars}\n",
    "\n",
    "# Normalize inputs\n",
    "norm_layers = []\n",
    "for v in input_vars:\n",
    "    min_max = np.array(CHANNEL_MIN_MAX[v]) # [2,]\n",
    "\n",
    "    # choose mean,var to get approximate [-1,1] scaling\n",
    "    var=((min_max[1]-min_max[0])/2)**2 # scalar\n",
    "    var=np.array(2*[var,])    # [n_sweeps,]\n",
    "    offset=(min_max[0]+min_max[1])/2    # scalar\n",
    "    offset=np.array(2*[offset,]) # [n_sweeps,]\n",
    "    \n",
    "    norm_layers.append(\n",
    "        keras.layers.Normalization(mean=offset, variance=var,\n",
    "                                   name='Normalized_%s' % v)\n",
    "    )\n",
    "\n",
    "# Concatenate normed inputs along channel dimension\n",
    "x=keras.layers.Concatenate(axis=-1,name='Concatenate1')(\n",
    "        [l(inputs[v]) for l,v in zip(norm_layers,input_vars)]\n",
    "        )\n",
    "\n",
    "# Replace background (nan) with -3\n",
    "x = FillNaNs(fill_val=-3,name='ReplaceNan')(x)\n",
    "\n",
    "# Processing\n",
    "x = keras.layers.Conv2D(32,3,padding='same',activation='relu')(x)\n",
    "# add more..\n",
    "x = keras.layers.Conv2D(1,1,padding='same',activation='relu',name='TornadoLikelihood')(x)\n",
    "y = keras.layers.GlobalMaxPool2D(name='GlobalMaxPool')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs,outputs=y,name='TornadoDetector')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dfbfa0-3cdc-4676-864e-f7cbcd869443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "opt  = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss=keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "model.compile(loss=loss, optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f402213-58a7-44be-a1cb-4d0aa16428d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "# steps_per_epoch=10 for demo purposes\n",
    "model.fit(ds,epochs=3,steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae798b07-9ac5-4784-ba44-cfa242649e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a test set\n",
    "ds_test = KerasDataLoader(data_root=data_root,\n",
    "                         data_type='test',\n",
    "                         years=[2018,2019],\n",
    "                         batch_size = 8, \n",
    "                         workers = 4,\n",
    "                         use_multiprocessing = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af44de5-af4e-438e-a0b1-153fbe72be84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "import tornet.metrics.keras.metrics as km\n",
    "metrics = [keras.metrics.AUC(from_logits=True,name='AUC'),\n",
    "           km.BinaryAccuracy(from_logits=True,name='BinaryAccuracy'), \n",
    "           ]\n",
    "model.compile(loss=loss,metrics=metrics)\n",
    "\n",
    "# steps=10 for demo purposes\n",
    "model.evaluate(ds_test,steps=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tornet-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
